{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abdullah Nasih Jasir (Абдулла Насих Джасир)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ALICE NLP**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **1. Download Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        chapter_title content\n",
      "0               CHAPTER I.     Down the Rabbit-Hole\\r        \n",
      "1                  CHAPTER II.    The Pool of Tears\\r        \n",
      "2      CHAPTER III.   A Caucus-Race and a Long Tale\\r        \n",
      "3   CHAPTER IV.    The Rabbit Sends in a Little Bi...        \n",
      "4          CHAPTER V.     Advice from a Caterpillar\\r        \n",
      "5                     CHAPTER VI.    Pig and Pepper\\r        \n",
      "6                    CHAPTER VII.   A Mad Tea-Party\\r        \n",
      "7         CHAPTER VIII.  The Queen’s Croquet-Ground\\r        \n",
      "8            CHAPTER IX.    The Mock Turtle’s Story\\r        \n",
      "9              CHAPTER X.     The Lobster Quadrille\\r        \n",
      "10              CHAPTER XI.    Who Stole the Tarts?\\r        \n",
      "11                  CHAPTER XII.   Alice’s Evidence\\r        \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# FETCH THE TEXT FROM URL\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(url)\n",
    "text_content = response.text\n",
    "\n",
    "# CHAPTERS SPLIT\n",
    "chapters = re.split(r'(CHAPTER\\s+[IVXLCDM]+\\.\\s+[^\\n]*)\\s*\\n', text_content)\n",
    "chapter_titles = chapters[1::2]\n",
    "chapter_contents = chapters[2::2]\n",
    "\n",
    "# TAKE IT TO DATAFRAME\n",
    "raw_df = pd.DataFrame({\"chapter_title\": chapter_titles, \"content\": chapter_contents})\n",
    "print(raw_df.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the contents list divide to independent row without the text in content column, we should to delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       chapter_title  \\\n",
      "0               CHAPTER I.\\r\\nDown the Rabbit-Hole\\r   \n",
      "1                 CHAPTER II.\\r\\nThe Pool of Tears\\r   \n",
      "2    CHAPTER III.\\r\\nA Caucus-Race and a Long Tale\\r   \n",
      "3  CHAPTER IV.\\r\\nThe Rabbit Sends in a Little Bi...   \n",
      "4          CHAPTER V.\\r\\nAdvice from a Caterpillar\\r   \n",
      "\n",
      "                                             content  \n",
      "0  Alice was beginning to get very tired of sitti...  \n",
      "1  “Curiouser and curiouser!” cried Alice (she wa...  \n",
      "2  They were indeed a queer-looking party that as...  \n",
      "3  It was the White Rabbit, trotting slowly back ...  \n",
      "4  The Caterpillar and Alice looked at each other...  \n"
     ]
    }
   ],
   "source": [
    "# DROP ROWS\n",
    "raw_df = raw_df.drop(raw_df.index[0:12])\n",
    "\n",
    "# RESET THE INDEX\n",
    "raw_df = raw_df.reset_index(drop=True)\n",
    "\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **2. Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization.**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# LOWERCASING\n",
    "preprocessed_df[\"content\"] = preprocessed_df[\"content\"].apply(lambda x: x.lower())\n",
    "\n",
    "# TOKENIZATION\n",
    "preprocessed_df[\"content\"] = preprocessed_df[\"content\"].apply(word_tokenize)\n",
    "\n",
    "# REMOVING NOISE\n",
    "def remove_noise(words):\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        cleaned_word = re.sub(r'[^A-Za-z\\s]+', '', word)\n",
    "        cleaned_word = cleaned_word.lower()\n",
    "        if cleaned_word:\n",
    "            cleaned_words.append(cleaned_word)\n",
    "    return cleaned_words\n",
    "\n",
    "preprocessed_df[\"content\"] = preprocessed_df[\"content\"].apply(remove_noise)\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "preprocessed_df[\"content\"] = preprocessed_df[\"content\"].apply(remove_stopwords)\n",
    "\n",
    "# LEMMATIZATION\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "preprocessed_df[\"content\"] = preprocessed_df[\"content\"].apply(lemmatize_words)\n",
    "\n",
    "# REJOINING TOKENS\n",
    "preprocessed_df[\"content\"] = preprocessed_df[\"content\"].apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **3. Find Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_df = preprocessed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# REMOVE THE ALICE WORD\n",
    "exploratory_df[\"content_cleaned\"] = exploratory_df[\"content\"].apply(lambda x: re.sub(r'\\balice\\b', '', x))\n",
    "exploratory_df = exploratory_df.drop(columns=[\"content\"])\n",
    "\n",
    "# VECTORIZE USING TFIDVECTORIZER\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(exploratory_df[\"content_cleaned\"])\n",
    "\n",
    "# CONVERT TFIDF TO DATAFRAME\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOP 10 WORDS AND THEIR SUITABLE TITLE FOR EACH CHAPTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little    0.173424\n",
      "bat       0.171089\n",
      "door      0.154574\n",
      "key       0.151132\n",
      "eat       0.143506\n",
      "like      0.127178\n",
      "think     0.127178\n",
      "way       0.127178\n",
      "either    0.123005\n",
      "see       0.115616\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 1\n",
    "chapter_1 = tfidf_df.iloc[0]\n",
    "top_10_words1 = chapter_1.nlargest(10)\n",
    "\n",
    "print(top_10_words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 1:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Little Door and the Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse     0.315662\n",
      "little    0.189146\n",
      "pool      0.169681\n",
      "swam      0.159762\n",
      "cat       0.157831\n",
      "dear      0.154499\n",
      "said      0.133515\n",
      "foot      0.129849\n",
      "mabel     0.127809\n",
      "go        0.122388\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 2\n",
    "chapter_2 = tfidf_df.iloc[1]\n",
    "top_10_words2 = chapter_2.nlargest(10)\n",
    "\n",
    "print(top_10_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 2:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"Little Mouse in the Pool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse      0.401868\n",
      "said       0.366934\n",
      "dodo       0.319406\n",
      "prize      0.185958\n",
      "lory       0.159703\n",
      "dry        0.141075\n",
      "thimble    0.123972\n",
      "know       0.118714\n",
      "bird       0.114819\n",
      "dinah      0.105521\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 3\n",
    "chapter_3 = tfidf_df.iloc[2]\n",
    "top_10_words3 = chapter_3.nlargest(10)\n",
    "\n",
    "print(top_10_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 3:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Mouse's Prize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill       0.215152\n",
      "window     0.210644\n",
      "little     0.201710\n",
      "rabbit     0.190681\n",
      "puppy      0.184313\n",
      "bottle     0.135677\n",
      "chimney    0.135677\n",
      "fan        0.135677\n",
      "glove      0.135677\n",
      "one        0.128361\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 4\n",
    "chapter_4 = tfidf_df.iloc[3]\n",
    "top_10_words4 = chapter_4.nlargest(10)\n",
    "\n",
    "print(top_10_words4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 4:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The little rabbit and a puppy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caterpillar    0.456587\n",
      "said           0.435911\n",
      "pigeon         0.288889\n",
      "serpent        0.288889\n",
      "egg            0.144444\n",
      "youth          0.144444\n",
      "size           0.114750\n",
      "father         0.103375\n",
      "little         0.092212\n",
      "well           0.083829\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 5\n",
    "chapter_5 = tfidf_df.iloc[4]\n",
    "top_10_words5 = chapter_5.nlargest(10)\n",
    "\n",
    "print(top_10_words5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 5:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Caterpillar and the Serpent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said       0.374080\n",
      "cat        0.338714\n",
      "footman    0.274285\n",
      "baby       0.215929\n",
      "mad        0.190743\n",
      "duchess    0.165527\n",
      "pig        0.157040\n",
      "wow        0.137143\n",
      "like       0.127346\n",
      "cook       0.121382\n",
      "Name: 5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 6\n",
    "chapter_6 = tfidf_df.iloc[5]\n",
    "top_10_words6 = chapter_6.nlargest(10)\n",
    "\n",
    "print(top_10_words6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 6:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The madness of baby cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatter      0.466133\n",
      "dormouse    0.431742\n",
      "said        0.382525\n",
      "hare        0.266249\n",
      "march       0.266249\n",
      "twinkle     0.148954\n",
      "time        0.110219\n",
      "tea         0.098877\n",
      "draw        0.095943\n",
      "clock       0.093096\n",
      "Name: 6, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 7\n",
    "chapter_7 = tfidf_df.iloc[6]\n",
    "top_10_words7 = chapter_7.nlargest(10)\n",
    "\n",
    "print(top_10_words7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 7:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Hatter and the Dormouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen          0.450239\n",
      "said           0.332164\n",
      "hedgehog       0.221839\n",
      "king           0.211481\n",
      "gardener       0.177471\n",
      "soldier        0.151466\n",
      "cat            0.150672\n",
      "five           0.133363\n",
      "executioner    0.133103\n",
      "procession     0.133103\n",
      "Name: 7, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 8\n",
    "chapter_8 = tfidf_df.iloc[7]\n",
    "top_10_words8 = chapter_8.nlargest(10)\n",
    "\n",
    "print(top_10_words8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 8:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The King and The Queen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said       0.413998\n",
      "turtle     0.411420\n",
      "mock       0.395596\n",
      "gryphon    0.284062\n",
      "duchess    0.204999\n",
      "moral      0.187724\n",
      "queen      0.164630\n",
      "went       0.094421\n",
      "never      0.079894\n",
      "say        0.078445\n",
      "Name: 8, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 9\n",
    "chapter_9 = tfidf_df.iloc[8]\n",
    "top_10_words9 = chapter_9.nlargest(10)\n",
    "\n",
    "print(top_10_words9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 9:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Queen Turtle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turtle       0.419633\n",
      "mock         0.379024\n",
      "gryphon      0.376653\n",
      "said         0.279597\n",
      "dance        0.231962\n",
      "lobster      0.231962\n",
      "beautiful    0.162439\n",
      "soup         0.162439\n",
      "join         0.160589\n",
      "whiting      0.142746\n",
      "Name: 9, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 10\n",
    "chapter_10 = tfidf_df.iloc[9]\n",
    "top_10_words10 = chapter_10.nlargest(10)\n",
    "\n",
    "print(top_10_words10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 10:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Dance of the Turtle and Gryphon\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king              0.407538\n",
      "hatter            0.366727\n",
      "said              0.320623\n",
      "court             0.296537\n",
      "dormouse          0.256998\n",
      "witness           0.230191\n",
      "queen             0.116798\n",
      "juror             0.115096\n",
      "officer           0.115096\n",
      "breadandbutter    0.098846\n",
      "Name: 10, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 11\n",
    "chapter_11 = tfidf_df.iloc[10]\n",
    "top_10_words11 = chapter_11.nlargest(10)\n",
    "\n",
    "print(top_10_words11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 11:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The King's Court\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said      0.468572\n",
      "king      0.395265\n",
      "jury      0.200168\n",
      "queen     0.148752\n",
      "sister    0.140117\n",
      "dream     0.135959\n",
      "would     0.119440\n",
      "slate     0.113300\n",
      "rabbit    0.109187\n",
      "fit       0.105541\n",
      "Name: 11, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CHAPTER 12\n",
    "chapter_12 = tfidf_df.iloc[11]\n",
    "top_10_words12 = chapter_12.nlargest(10)\n",
    "\n",
    "print(top_10_words12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAPTER 12:  \n",
    "Based on those words (Top 10 of most common words in that chapter), I think that chapter will have title \"The Queen's Dream\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Find the Top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abdullah\n",
      "[nltk_data]     NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Abdullah NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abdullah\n",
      "[nltk_data]     NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most used verbs in sentences with 'Alice':\n",
      "said: 256\n",
      "was: 181\n",
      "had: 98\n",
      "be: 81\n",
      "s: 51\n",
      "thought: 50\n",
      "have: 43\n",
      "do: 41\n",
      "is: 41\n",
      "went: 41\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# EXTRACT THE SENTENCE\n",
    "sentences_with_alice = [sentence for sentence in re.split(r'(?<=[.!?])\\s+', text_content) if 'Alice' in sentence]\n",
    "\n",
    "# TOKENIZE\n",
    "all_verbs = []\n",
    "for sentence in sentences_with_alice:\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # EXTRACT THE VERB (AS I KNOW, VB IS ONE OF THE VERB TAGS IN POS TAGGING)\n",
    "    verbs = [word.lower() for word, tag in pos_tags if tag.startswith('VB') and word.isalpha()]\n",
    "    all_verbs.extend(verbs)\n",
    "\n",
    "# FIND THE TOP 10 MOST COMMON VERBS\n",
    "top_verbs = Counter(all_verbs).most_common(10)\n",
    "\n",
    "print(\"Top 10 most used verbs in sentences with 'Alice':\")\n",
    "for verb, freq in top_verbs:\n",
    "    print(f\"{verb}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on here, The thing that Alice does most often is \"said\" or Yapping or Talking."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
